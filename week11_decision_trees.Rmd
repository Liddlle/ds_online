---
title: "Week11_final"
output: html_document
---

 Предположим, перед вами стоят следующие задачи:

* есть растения нескольких сортов, нужно выделить признаки, характеризующие каждый сорт
* есть ряд диагнозов, необходимо определить симптомы, соответствующие диагнозу
* есть база клиентов с указанием, кто продолжает пользоваться сервисоами компании, а кто ушел. Нужно выделить признаки, которые позволят выявить "проблемных" клиентов

Подобные задачи делятся на две групы: задачи регрессии и задачи классификации. Задача классификации возникает  в том случае, если интересующим нас показателем является принадлежность в некоторому классу (болен/не болен, спам/не спам, ушел/остался, высокий/средний/низкий...) -- т.е. категориальная переменная. 

Про задачу регрессии мы говорим тогда, когда мы хотим предсказать некий количественный, числовой показатель (уровень продаж, рейтинг, риск заразиться...).

Рассмотрим, как подобные задачи можно решать с помощью R.

Первый датасет, с которым мы будем работать содержит данные о пассажирах Титаника. О датасете https://www.kaggle.com/c/titanic/data
Когда Титаник затонул, 1502 из 2224 пассажиров и членов экипажа погибли. Одна из главных причин такого большого числа жертв - нехватка спасательных жилетов на корабле. Те, кто смотрел фильм, знают, что шанс одних пассажиров выжить был больше (Роуз), чем других (бедный Джек). Далее мы применим техники машинного обучения для предсказания шанса выжить для каждого пассажира.

Загрузим данные и разделим их на **обучающую (train)** и **тестовую (test)** выборки. На первой вы будете строить модель, а на второй оценивать качество предсказания. Тестовая выборка будет содержать 20% данных

```{r message=FALSE, warning=FALSE}
titanic <- read.csv("/principal/courses/minor2_2016/lab10-tree2/Titanic.csv")
str(titanic)

titanic$Pclass <- as.factor(titanic$Pclass)
titanic$Survived <- as.factor(titanic$Survived)
```
Сначала посчитаем, сколько строк составят 20% от данных
```{r message=FALSE, warning=FALSE}
test_size = nrow(titanic)*0.20
test_size
```

Теперь отберем `r test_size` номеров строк случайным образом
```{r message=FALSE, warning=FALSE}
set.seed(1238)
test_indices = sample(1:dim(titanic)[1], test_size)
head(test_indices, 20)
```

Затем в качестве тестовой выборки возьмем те строки, номера которых "выпали"

```{r message=FALSE, warning=FALSE}
test_data = titanic[test_indices,]
```

А в качестве обучающей --  все, кроме них

```{r message=FALSE, warning=FALSE}
train_data = titanic[-test_indices,]
```
Как много людей в вашей тренировачной выборке выжили? Чтобы узнать это используйте команду table() в комбинации с $-оператором для выбора колонки из таблицы.

```{r message=FALSE, warning=FALSE}
# Число выживших в абсолютных числах
table(train_data$Survived)

# В пропорциях
prop.table(table(train_data$Survived))
```
Теперь видно, что  439 человек погибли (62%) и 274 выжили (38%). 

В целом, команда table() может помочь вам выявить какие переменные имеют предсказательную силу. Например, может быть пол имеет значение для предсказания? Для включения гендера в сравнение используйте следующий код

```{r message=FALSE, warning=FALSE}
table(train_data$Sex, train_data$Survived)
```
Для получения пропорций заверните table() в prop.table(), но нужно будет уточнить какую пропорцию вы хотите: построчную или колоночную. Для этого включите в prop.table() аргумент margin и определите его значение как 1 или 2 соответственно.
```{r message=FALSE, warning=FALSE}
prop.table(table(train_data$Sex, train_data$Survived), 1)
```
Еще одна переменная, которая может повлиять на выживаемость - возраст: возможно, детей спасали в первую очередь. Вы проверите это, создав категориальную переменную child. 
```{r message=FALSE, warning=FALSE}
# Создаем переменную
train_data$Child <- NA
train_data$Child[train_data$Age < 18] <- 1
train_data$Child[train_data$Age >= 18] <- 0

# Двусторонее сравнение
prop.table(table(train_data$Child, train_data$Survived), 1)
```
До настоящего момента вы проделали определенные манипуляции, чтобы найти сабсеты, имеющие больший шанс выжить (женщины и дети). Дерево решений автоматизирует этот процесс и в результате вы получаете блок-схему, которая легко интерпретируется. 

Концептуально алгоритм дерева решений делит исходный набор данных на подмножества, которые становятся все более и более гомогенными относительно определенных признаков, в результате чего формируется древовидная иерархическая структура. Алгоритм начинает со всех данных в корневом узле и сканирует все переменные, выбирая лучшую из них для разделения. Как только переменная выбрана, происходит разделение и переход на один уровень (или узел) ниже, далее эта процедура повторяется. Конечные узлы дерева называются терминальными (terminal nodes) или листьями (leaves).

Для построения первого дерева решений мы будем использовать пакет rpart.
Подробнее про пакет бы можете прочитать здесь https://cran.r-project.org/web/packages/rpart/rpart.pdf

```{r message=FALSE, warning=FALSE}
library(rpart)
```
Внутри rpart, есть функция rpart(), с помощью которой мы и построим дерево решений. Функция включает следующие аргументы:

**formula:** специфицирует интересующую переменную (Dependent variable) и переменные, используемые для предсказания (Independent variables) 

например, formula = Survived ~ Sex + Age

Если хотите построить дерево, используя все переменные, то после знака "~" поставьте точку, вы получите следующую формулу formula = Survived ~.

**data:** датасет для построения дерева (у нас это train_data).

**method:** тип предсказания. Мы хотим предсказать категориальную переменную, это задача классификации: method = "class".

Чтобы визуализировать полученное дерево вы можете использовать prp(my_tree) из пакета rpart.plot. 
```{r message=FALSE, warning=FALSE}
# Build the decision tree
my_tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_data, method = "class")

# Визуализируем дерево решений
library(rpart.plot)
prp(my_tree)

# Добавим больше информативности
prp(my_tree, type=2, extra=104)
prp(my_tree, type=2, extra=104, nn=TRUE, fallen.leaves=TRUE,faclen=0, varlen=0, shadow.col="grey", branch.lty=3)
```

Каждый терминальный узел помечен предсказанным классом (0 - не выжил, 1 - выжил). Все узлы размещены по принципу падающих листьев (fallen.leaves=TRUE), чтобы визуально усилить структуру. Мы видим прямые линии сверху вниз, что позволяет быстро принять решение. Каждый узел содержит вероятность для каждого класса и процент наблюдений ассоциируемый с узлом (extra=104). Включены номера узлов (nn=TRUE), это позволяет перекрестно ссылаться на каждый узел дерева. Использование пунктирной линии (branch.lty=3) переключает внимание с тяжелых линий и фокусирует его на узлах, при этом все еще четко можно проследить связи. Серая тень - приятное дополнение.

Множество способов визуализации с помощью prp() описано здесь http://www.academia.edu/6241482/Data_Science_with_R_Decision_Trees

Основываясь на построенном дереве решений определите, какие переменные играют наибольшую роль в определении того выживет ли пассажир?

**Варианты ответа**

Sex, Age, Pclass, SibSp, Parch, Fare and Embarked

Sex, Age, Pclass, SibSp, Embarked, Fare - верный

Sex, Age, Parch and Embarked

Sex, Age and Embarked

Следующим шагом нам нужно предсказать количество выживших для наблюдений в тестовой выборке. Для этого мы воспользуемся функцией predict(). Здесь, my_tree это модель, которую вы построили, test_data это датасет, для которого вы делаете предсказания
```{r message=FALSE, warning=FALSE}
my_prediction <- predict(my_tree, newdata = test_data, type = "class")
my_prediction
```

Посчитаем точность модели, для этого воспользуемся confusion table
```{r message=FALSE, warning=FALSE}
# confusion table
table(my_prediction, test_data$Survived)
(100+42)/178
```

Точность модели - 80%

Как уже было продемонстрировано с переменной child, вы можете самостоятельно создавать переменные и включать их в модель. Давайте попробуем увеличить точность модели, добавив family_size. Предположение заключается в том, что большим семьям требуется больше времени чтобы собраться вместе на тонущем корабле, соответвтвенно у них шанс выжить меньше. Размер семьи определяется переменными SibSp и Parch, которые отражают количество членов семьи, с которыми путешествует каждый конкретный пассажир. Для создания новой переменной family_size, мы суммируем SibSp и Parch плюс один (само наблюдение).
```{r message=FALSE, warning=FALSE}
train_data$family_size <- train_data$SibSp + train_data$Parch + 1
test_data$family_size <- test_data$SibSp + test_data$Parch + 1

my_tree_two <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Fare + Embarked + family_size,
                      data = train_data, 
                      method = "class")



my_prediction2 <- predict(my_tree_two, newdata = test_data, type = "class")
```
```{r message=FALSE, warning=FALSE}
prp(my_tree_two, type=2, extra=104)
```

Как видите, наша новая переменная не появилась в дереве
```{r message=FALSE, warning=FALSE}
table(my_prediction2, test_data$Survived)
(104+42)/178
```

Точность 82% - качество предсказания улучшилось. Однако обратите внимание, что рост показателя может происходить просто в результате добавления новой переменной, она не появилась в структуре дерева, значит не так важна для предсказания как содержащиеся в дереве.

Мы можем улучшить показатель, сделав модель более сложной. В rpart количество деталей опредлеляется двумя параметрами:

**cp**(complexity parameter) - это метрика, которая останавливает ветвление дерева, когда оно уже не улучшает показатели модели. 

**minsplit** определяет минимальное количество наблюдений, которые должны содержаться в листе дерева

```{r message=FALSE, warning=FALSE}
# Построим дерево с минимальным количество наблюдений в каждом листе 5 и с cp = 0.006
my_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_data, method = "class", control = rpart.control(minsplit = 5, cp = 0.006))

# Визуализируем
prp(my_tree_three)
```

Таким образом мы постороили дерево классификации. Для построение дерева с помощью rpart требуется tree pruning - отсечение ветвей там, где эта процедура не приводит к серьезному возрастанию ошибки. Подробнее читайте здесь http://r-analytics.blogspot.ru/2017/02/blog-post.html#.WZBlq-zyjIV

Для построения regression decision tree мы будем использовать пакет partykit 
https://cran.r-project.org/web/packages/partykit/partykit.pdf  и таблицу algae, включенный в пакет DMwR и содержащую данные гидробиологических исследований обилия водорослей в различных реках. Каждое из 200 наблюдений содержит информацию о 18 переменных, в том числе:

три номинальных переменных, описывающих размеры size = c("large", "medium", "small") и скорость течения реки speed = c("high", "low", "medium"), а также время года season = c("autumn", "spring", "summer", "winter"), сопряженное с  моментом взятия проб;

8 переменных, составляющих комплекс наблюдаемых гидрохимических показателей: максимальное значение рН  mxPH (1), минимальное содержание кислорода mnO2 (2), хлориды Cl (10), нитраты NO3 (2), ионы аммония NH4 (2), орто-фосфаты oPO4 (2), общий минеральный фосфор PO4 (2) и число клеток хлорофилла Chla (12); в скобках приведено число пропущенных значений;

средняя численность каждой из 7 групп водорослей a1-a7 (видовой состав не идентифицировался).
```{r message=FALSE, warning=FALSE}
library(partykit)
algae = DMwR::algae
```

Построим модель для предсказания численности водорослей вида а1, используя для этого все доступные переменные

Разделим выборку снова на две части
```{r message=FALSE, warning=FALSE}
set.seed(2)
train=sample(1:nrow(algae), 100)
algae.test <- algae[-train,]
algae.train <- algae[train,]

tree <- ctree(a1 ~ ., data = algae.train)
```

Визуализируем получившееся дерево
```{r message=FALSE, warning=FALSE}
plot(tree)

plot(tree, type = "simple")

plot(tree, gp = gpar(fontsize = 6),     # font size changed to 6
      inner_panel=node_inner,
      ip_args=list(
        abbreviate = F, 
        id = FALSE,
        pval=F)
 )

plot(tree, gp = gpar(fontsize = 8),     # font size changed to 8
      inner_panel=node_inner,
      ip_args=list(
        abbreviate = F, 
        id = FALSE,
        pval=F),
     type = "simple"
 )
```
```{r}
tree.pred <- predict(tree, algae.test)
```
Для того чтобы понять насколько наше предсказание отличается от реального значения посчитаем коэффициент детерминации. Коэффициент детерминации (R2) дает предварительную оценку качества модели. Он показывает долю объясненной дисперсии зависимой переменной (доля общей суммы квадратов, объясненной уравнением регрессии).

```{r message=FALSE, warning=FALSE}
# residual sum of squares (rss)
rss = sum((tree.pred - algae.test$a1)^2) 
# total sum of squares (tss)
tss = sum((mean(algae.test$a1) - algae.test$a1)^2)
# R^2
1-rss/tss
```
R2 принимает значение от 0 до 1, чем ближе R2 к 1, тем лучше модель, тем меньше доля необъяснённого. Наша модель объясняет 25% дисперссии в средней численности водорослей вида а1

Возможные проблемы: Проблемы с использованием R2 заключаются в том, что его значение не уменьшается при добавлении в уравнение факторов, сколь плохи бы они ни были. Он гарантированно будет равен 1, если мы добавим в модель столько факторов, сколько у нас наблюдений. Поэтому сравнивать модели с разным количеством факторов, используя R2, не имеет смысла.


**Доп. материалы:**

В следующей статье анализируются фукции rpart() из пакета rpart и ctree() из пакета party https://www.r-bloggers.com/party-with-the-first-tribe/

http://www.statmethods.net/advstats/cart.html
